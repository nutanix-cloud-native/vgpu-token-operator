# Copyright 2025 Nutanix. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

controllerManager:
  replicas: 1
  container:
    image:
      repository: controller
      tag: "{{ .Chart.AppVersion }}"
    resources:
      limits:
        cpu: 500m
        memory: 128Mi
      requests:
        cpu: 10m
        memory: 64Mi
    livenessProbe:
      initialDelaySeconds: 15
      periodSeconds: 20
      httpGet:
        path: /healthz
        port: 8081
    readinessProbe:
      initialDelaySeconds: 5
      periodSeconds: 10
      httpGet:
        path: /readyz
        port: 8081
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - "ALL"
  securityContext:
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  terminationGracePeriodSeconds: 10
  serviceAccountName: vgpu-token-operator-controller-manager

vgpuCopy:
  image: "ghcr.io/nutanix-cloud-native/vgpu-token-copier:{{ .Chart.AppVersion }}"

# Configuration for vGPU Token Custom Resources (VGPUToken).
#
# This allows you to define multiple vGPU token instances, each corresponding
# to a VGPUToken custom resource in Kubernetes.
#
# To add a new vGPU token:
# - Add a new entry under 'vgpuTokenObjects'.
# - Each entry requires:
#   - 'tokenName': The desired name for the VGPUToken custom resource.
#   - 'tokenSecretName': The name of the Kubernetes Secret that holds the
#                        actual vGPU token value (e.g., "my-vgpu-token-secret").
#                        This secret must exist in the same namespace.
# - Optional fields:
#   - 'hostDirectoryPath': Specifies a host directory path for the vGPU token
#                          that the grid service will read from. *NOTE: You likely
#                          will not need to set this field.*
#                          (e.g., "/mnt/vgpu-tokens/my-token")
#   - 'nodeSelector': A map of key-value pairs to select specific nodes
#                     where this vGPU token should be applied.
#                     By default, the token is deployed with nodeSelector
#                     { nvidia.com/vgpu.present: "true" }
#                     This can be helpful to set if you have different
#                     tokens for different sets of nodes.
#                     If all of your nodes are OK to use the same token, you
#                     can use the default and leave this field blank.
#                     *NOTE: if you set this remember to set nvidia.com/vgpu.present: "true"
#                     as another set, becaues it is not automatically added.
# Example:
# vgpuTokenObjects:
#   - tokenName: my-first-vgpu-cr
#     tokenSecretName: my-first-vgpu-secret
#     hostDirectoryPath: "/opt/nutanix/vgpu/token1"
#     nodeSelector:
#       kubernetes.io/hostname: "prod-gpu-node-01"
#       nvidia.com/vgpu.present: "true"
#   - tokenName: another-vgpu-cr
#     tokenSecretName: another-vgpu-secret
#
vgpuTokenObjects: []

rbac:
  enable: true

# [CRDs]: To enable the CRDs
crd:
  # This option determines whether the CRDs are included
  # in the installation process.
  enable: true

  # Enabling this option adds the "helm.sh/resource-policy": keep
  # annotation to the CRD, ensuring it remains installed even when
  # the Helm release is uninstalled.
  # NOTE: Removing the CRDs will also remove all cert-manager CR(s)
  # (Certificates, Issuers, ...) due to garbage collection.
  keep: true

# [METRICS]: Set to true to generate manifests for exporting metrics.
# To disable metrics export set false, and ensure that the
# ControllerManager argument "--metrics-bind-address=:8443" is removed.
metrics:
  enable: true

# [PROMETHEUS]: To enable a ServiceMonitor to export metrics to Prometheus set true
prometheus:
  enable: false

# [CERT-MANAGER]: To enable cert-manager injection to webhooks set true
certmanager:
  enable: false

# [NETWORK POLICIES]: To enable NetworkPolicies set true
networkPolicy:
  enable: false
